### This is START of the README.md file:
~~~
# AI Demo Project Using SDE Sandbox

## Project Overview

Welcome to our AI Demo Project, showcasing the integration and application of AI technologies in Python within SDE sandbox. This project features capabilities for generating articles using OpenAI's API, and for fetching, storing, and searching web content using [Chroma DB](https://docs.trychroma.com/).

### Main Features:
- **Sitemap Fetching and Parsing**: Automates the process of fetching sitemap XML from websites and parsing it to extract URLs.
- **Content Extraction and Storage**: Retrieves and stores web page content in ChromaDB, a versatile database system.
- **AI-Powered Search**: Employs OpenAI's embedding functions for efficient and intelligent content search within ChromaDB.
- **Article Generation**: Generates articles based on user prompts and search terms, utilizing OpenAI's GPT-4 turbo model.

## Getting Started

### Prerequisites
- Access to an SDE such as Daytona.io which supports Dev Container Specification
- Python >3.10
- OpenAI API key

### Installation
There are two ways to set up the environment for this project:

1. **Using an SDE**:
   - Users of Daytona.io, Codeanywhere, Codespaces, or VS Code can auto-configure their environment with devcontainer.json, allowing for instant start.
   - Just point the SDE to the Git repository url: https://github.com/nkkko/ai-sandbox-demo
   - Set up an `.env` file with your `OPENAI_API_KEY`.

2. **Manual Setup**:
   - Clone the repository to your workspace or local environment.
   - Install necessary packages:
     ```
     pip install -r requirements.txt
     ```
     Or run:
     ```
     pip install openai chromadb python-dotenv bs4 argparse lxml
     ```
   - Set up an `.env` file with your `OPENAI_API_KEY` or don't and just use the default embeddings function (all-MiniLM-L6-v2).

### Usage

1. **populate.py**: 
   - Run the script with a sitemap URL to fetch, parse, and store website content. Optionally select the embeddings function to use with `--ef`.
   - Usage: `python populate.py [SITEMAP_URL] [--n [MAX_URLS]] [--ef {default|openai}]`
   - Example: `python populate.py https://www.example.com/sitemap.xml --n 100 --ef openai`

2. **search.py**:
   - Perform searches in the stored data. Optionally select the embeddings function to use with `--ef`.
   - Usage: `python search.py [SEARCH_QUERY] [--n [NUMBER_OF_RESULTS]] [--ef {default|openai}]`
   - Example: `python search.py "example search query" --n 5 --ef default`

3. **write.py**:
   - Generate articles based on a prompt and ChromaDB search terms. Optionally select the embeddings function to use with `--ef`.
   - Usage: `python write.py [PROMPT] --s [SEARCH_TERM] --n [NUMBER_OF_RESULTS] [--ef {default|openai}]`
   - Example: `python write.py "Write an article about AI" --s "artificial intelligence" --n 3 --ef openai`

The `--ef` argument allows you to specify which [embeddings function](https://docs.trychroma.com/embeddings) to use when interacting with ChromaDB. By default, `default_ef` is used (all-MiniLM-L6-v2). If you want to use OpenAI's embeddings, specify `--ef openai`.

## Structure

- `SynthSpider.py`: Core module containing the logic for fetching, parsing, storing, and searching.
- `populate.py`: Script for populating ChromaDB with content from a sitemap.
- `search.py`: Script to search within the stored data.
- `write.py`: Script to generate articles using OpenAI and ChromaDB to fetch context.
- `db/`: Directory containing ChromaDB client and utilities.
- `.env`: Environment file for storing your OpenAI API key.
- `.devcontainer`: Configuration directory with file for setting up the development environment automatically in supported SDEs.

## Examples

- Fetch and store 10 first articles from the sitemap:
  ```bash
    python populate.py https://www.daytona.io/sitemap-definitions.xml --ef azure --n 10
  ```

- Search for the first two terms that are closest to the query in the stored content:
  ```bash
    python search.py --ef azure --n 1 "sde"
  ```

- Generate an article within set context of the first result from the vector DB: 
  ```bash
    python write.py "Tell me a joke about" --s "guardrails" --ef azure --n 1
  ```
  ```bash
    Why was the developer afraid to play cards with the guardrails?

    Because every time they tried to deal, the guardrails kept reminding them to stay within their limits! ðŸš§ðŸ˜„
  ```

## Contributing
Contributions to enhance this demo project are welcomed. Please adhere to standard coding practices and provide documentation for your contributions.

## License
[MIT License](LICENSE.md)

## Acknowledgments
- Thanks to OpenAI for fantastic embeddings.
- Appreciation to the developers of ChromaDB.

## Future
- load system prompt and user prompt from file
- implement azure openai new endpoints for gpt4 turbo

---

*This README is part of an AI Demo Project using SDE as a sandbox environment for AI projects. It aims to guide users through the setup, usage, and contribution to the project.*


python write.py "Imagine you are experienced author and writer in dev tools space. Write a short form thought leadership opinion. It needs to have less less than 200 words. These are examples of the style of opinions that you can emulate: [1. In the quest for an efficient cloud development Integrated Development Environment (IDE), the initial and crucial step involves selecting the IDE that aligns seamlessly with project needs, financial considerations, and one's skill proficiency. The diverse landscape of IDEs offers options catering to various languages, frameworks, and development styles. Careful consideration of factors like scalability, collaboration features, and integration capabilities ensures the chosen IDE enhances productivity, aligns with budget constraints, and accommodates skill levels. This strategic selection lays the foundation for a streamlined development workflow, fostering an environment conducive to innovation and success. 2. To optimize your cloud development IDE, start by selecting one that aligns with your project's needs, budget, and skill level. Consider factors like programming languages, frameworks, cloud providers, customization options, collaboration features, and reliability. Compare options based on these criteria to find the IDE that best suits your cloud development goals. 3. Post selecting your cloud IDE, optimize performance, security, and usability via configuration. Tailor the interface with theme, layout, and shortcut adjustments to align with your workflow. Enhance coding efficiency and readability by tweaking code editor, syntax highlighting, auto-completion, and formatting options. Streamline the development lifecycle with debugging, testing, and deployment tools. Bolster security through authentication and encryption features, safeguarding code and data. Regularly review and update backup, recovery, and version control options to align with best practices, ensuring your cloud IDE remains finely tuned and resilient throughout the development journey. 4. I would always suggest using an IDE that is common in most working environments, which means either VS Code, one of the JetBrains family products, or a browser version of them. It seems like a waste of time and energy to learn a new IDE that you will never use outside of that project. 5. Enhancing your cloud IDE involves leveraging extensions and pluginsâ€”additional software components offering extra features, tools, and integrations. These augment your IDE's capabilities, supporting a broader range of programming languages, frameworks, and tools relevant to your project. Extensions and plugins also facilitate integration with other cloud services and applications, improving user experience and productivity. Explore and install extensions that align with your cloud development goals, providing valuable features like code snippets, templates, themes, or widgets to optimize your development workflow.] Now write original opinion on: [what else to consider about cloud IDEs, share examples, stories, or insights ] using the following context to support your writing: " --s "cloud development environment" --n 5
~~~
### This is END of the file README.md!

### This is START of the SynthSpider.py file:
~~~
# pip install openai chromadb python-dotenv bs4 argparse lxml tiktoken
import logging
import asyncio
from bs4 import BeautifulSoup
import chromadb
from chromadb.db.base import UniqueConstraintError  # Import the exception
from chromadb.utils import embedding_functions
from dotenv import load_dotenv
from openai import AzureOpenAI, OpenAI, RateLimitError
import backoff
import os
import requests
import tiktoken
import time
import xml.etree.ElementTree as ET

# Get the root logger
logger = logging.getLogger()
logger.setLevel(logging.INFO)  # Set the logging level to INFO

# Load environment variables
load_dotenv()

# Read the API key from the environment variable
azure_api_version = os.getenv("AZURE_API_VERSION")
azure_endpoint = os.getenv("AZURE_ENDPOINT")
azure_api_key = os.getenv("AZURE_OPENAI_API_KEY")

# OpenAI API Key
openai_api_key = os.getenv("OPENAI_API_KEY")

model_name = os.getenv("MODEL_NAME")

# Configure OpenAI Client
openai_client = OpenAI(api_key=openai_api_key)

# Configure Azure OpenAI Client
azure_openai_client = AzureOpenAI(
    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning
    api_version=azure_api_version,
    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource
    azure_endpoint=azure_endpoint,
    api_key = azure_api_key,
)

# Load the encoding once at the start of your script
encoding = tiktoken.encoding_for_model("text-embedding-ada-002")

# Embeddings functions
default_ef = embedding_functions.DefaultEmbeddingFunction()
openai_ef = embedding_functions.OpenAIEmbeddingFunction(
                api_key=openai_api_key,
                model_name="text-embedding-ada-002"
            )
azure_ef = embedding_functions.OpenAIEmbeddingFunction(
                api_key=azure_api_key,
                api_base=azure_endpoint,
                api_type="azure",
                api_version=azure_api_version,
                model_name="text-embedding-ada-002"
            )

# Initialize ChromaDB Client
# chroma_client = chromadb.Client() # in-memory db
chroma_client = chromadb.PersistentClient(path="db") # persistent db
collection_name = "sitemap_collection"

async def fetch_sitemap(url):
    """
    Asynchronously fetch a sitemap from the given URL.
    Returns the sitemap's XML content as a string, or None if an error occurs.
    """
    try:
        response = await asyncio.to_thread(requests.get, url)
        response.raise_for_status()
        return response.text
    except requests.RequestException as e:
        logging.error(f"Error fetching sitemap: {e}")
        return None

def parse_sitemap(sitemap_content, max_urls=None):
    """
    Parse the sitemap content and extract a limited number of URLs.

    Args:
    sitemap_content (str): XML content of the sitemap.
    max_urls (int, optional): Maximum number of URLs to extract. If None, extracts all URLs.

    Returns:
    List[str]: A list of extracted URLs, limited to 'max_urls' if specified.
    """

    # Parse the XML content
    tree = ET.ElementTree(ET.fromstring(sitemap_content))
    root = tree.getroot()

    # Define the namespace map for parsing sitemap XML
    # Adjusted to handle both http and https namespaces
    namespaces = {
        'http': 'http://www.sitemaps.org/schemas/sitemap/0.9',
        'https': 'https://www.sitemaps.org/schemas/sitemap/0.9'
    }

    # Try to extract the URLs with http namespace first
    urls = [element.text for element in root.findall('.//http:loc', namespaces)]
    
    # If no URLs found, try with https namespace
    if not urls:
        urls = [element.text for element in root.findall('.//https:loc', namespaces)]

    # Limit the number of URLs if max_urls is specified
    if max_urls is not None:
        urls = urls[:max_urls]

    return urls

async def fetch_and_save_html(url, update_progress, collection):
    """
    Fetch the HTML content of a given URL, extract and clean text from main content elements, 
    and save it to ChromaDB.
    """
    try:
        response = await asyncio.to_thread(requests.get, url)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'lxml')
        
        # Remove script and style elements
        for script_or_style in soup(['script', 'style', 'header', 'footer', 'nav']):
            script_or_style.extract()
        
        # Extract main content using common content markers
        main_content = soup.find_all(['article', 'main', 'div'], class_=lambda x: x and 'content' in x)
        
        # If no common content markers found, fall back to extracting all text
        if not main_content:
            main_content = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p'])
        
        text_content = ' '.join(element.get_text(strip=True, separator=' ') for element in main_content)

        save_to_chromadb(url, text_content, collection)
        update_progress()
    except requests.RequestException as e:
        logging.error(f"Error fetching HTML content from {url}: {e}")

# Global variables to keep track of the rate limit
tokens_per_minute_limit = 240000  # Your rate limit
tokens_used_this_minute = 0
minute_window_start = time.time()

def count_tokens(text):
    return len(encoding.encode(text))

def within_rate_limit(num_tokens):
    global tokens_used_this_minute, minute_window_start
    current_time = time.time()
    if current_time - minute_window_start >= 60:
        # Reset the count every minute
        tokens_used_this_minute = 0
        minute_window_start = current_time
    return tokens_used_this_minute + num_tokens <= tokens_per_minute_limit

def wait_for_rate_limit_reset():
    global minute_window_start
    time_to_wait = 100 - (time.time() - minute_window_start)
    if time_to_wait > 0:
        logging.info(f"Rate limit exceeded, waiting for {time_to_wait} seconds.")
        time.sleep(time_to_wait)
    # Reset the token count and window start time
    tokens_used_this_minute = 0
    minute_window_start = time.time()

@backoff.on_exception(backoff.expo, RateLimitError, max_tries=8, max_time=300)  # Increase max_time if needed
def save_to_chromadb(url, html_content, collection):
    global tokens_used_this_minute
    num_tokens = count_tokens(html_content)

    if not within_rate_limit(num_tokens):
        wait_for_rate_limit_reset()

    try:
        collection.upsert(
            documents=[html_content],
            metadatas=[{"url": url}],
            ids=[url]
        )
        tokens_used_this_minute += num_tokens
    except UniqueConstraintError as e:
        logging.warning(f"Duplicate entry for {url} not added to ChromaDB: {e}")
    except RateLimitError as e:
        logging.error(f"Rate limit exceeded when adding/updating {url} in ChromaDB: {e}")
        # Wait for the full duration of the rate limit reset window
        logging.info("Waiting for the rate limit to reset before retrying...")
        wait_for_rate_limit_reset()
        raise  # Re-raise the exception to trigger the backoff
    except Exception as e:
        logging.error(f"Exception while adding/updating {url} in ChromaDB: {type(e).__name__}, {e}")
        snippet = html_content[:200]
        logging.info(f"Content snippet: {snippet}")

def search_in_chromadb(query, n_results, collection):
    """
    Search in ChromaDB for the given query.

    Args:
    query (str): The search query.
    n_results (int): Number of search results to return.

    Returns:
    List of search results.
    """

    # Search in the collection
    search_results = collection.query(
        query_texts=[query],
        n_results=n_results
    )

    return search_results

def write_article(prompt):
    """
    Generate an article using the OpenAI API.
    """
    try:
        response = azure_openai_client.chat.completions.create(
            model=model_name,
            messages=[
                {"role": "system", "content": "Follow user instructions. Write using Markdown."},
                {"role": "user", "content": prompt}
            ]
        )
        # Access the 'content' attribute of the last message in the response
        last_message_content = response.choices[0].message.content
        return last_message_content

    except Exception as e:
        print(f"An error occurred during article generation: {e}")
        return None
~~~
### This is END of the file SynthSpider.py!

### This is START of the write.py file:
~~~
from SynthSpider import search_in_chromadb, write_article, default_ef, openai_ef, azure_ef, chroma_client, collection_name
import argparse

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Generate an article using OpenAI and ChromaDB.')
    parser.add_argument('prompt', type=str, help='Article prompt.')
    parser.add_argument('--s', type=str, required=True, help='Search term for ChromaDB.')
    parser.add_argument('--n', type=int, default=5, help='Number of ChromaDB search results.')
    parser.add_argument('--ef', type=str, default='default', choices=['default', 'openai', 'azure'], help='Embedding function to use (default or openai).')
    
    args = parser.parse_args()

    # Select the embedding function based on the user input
    if args.ef == "openai":
        embedding_function = openai_ef
    elif args.ef == "azure":
        embedding_function = azure_ef
    else:
        embedding_function = default_ef

    # Initialize the collection with the selected embedding function
    collection = chroma_client.get_or_create_collection(
        name=collection_name,
        embedding_function=embedding_function
    )

    # Search in ChromaDB
    search_results = search_in_chromadb(args.s, args.n, collection)

    # Process and append search results to the prompt
    additional_context = ""
    for i in range(len(search_results['documents'])):  # Iterate over each set of documents (outer list)
        for j in range(len(search_results['documents'][i])):  # Iterate over documents in each set (inner list)
            additional_context += f"{search_results['documents'][i][j]}\n\n"

    combined_prompt = args.prompt + "\n\n" + additional_context

    # Generate the article
    article = write_article(combined_prompt)
    print(article)

~~~
### This is END of the file write.py!

### This is START of the embeddings.py file:
~~~
import os
import json
import ipdb
import replicate
from tqdm.auto import tqdm
import chromadb
import csv


client = chromadb.PersistentClient(path="db/")

collection = client.get_or_create_collection(name=f"sitemap_collection")


results = collection.get(include=["documents", "embeddings", "metadatas"])

metadatas = [
    {"id": id, "document": doc, **meta}
    for id, doc, meta in zip(results["ids"], results["documents"], results["metadatas"])
]

with open("./embeddings.tsv", "w", encoding="utf-8") as f:
    for embedding in results["embeddings"]:
        f.write("\t".join(map(str, embedding)) + "\n")

# Extracting keys from the first item of combined_metadata as headers
headers = metadatas[0].keys()

with open("./metadata.tsv", "w", newline="", encoding="utf-8") as f:
    writer = csv.DictWriter(f, fieldnames=headers, delimiter="\t")
    writer.writeheader()
    for data in metadatas:
        writer.writerow(data)
~~~
### This is END of the file embeddings.py!

### This is START of the search.py file:
~~~
from SynthSpider import search_in_chromadb, default_ef, openai_ef, azure_ef, chroma_client, collection_name
import argparse

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Search in ChromaDB.')
    parser.add_argument('query', type=str, help='Search query.')
    parser.add_argument('--n', type=int, default=5, help='Number of results to return.')
    parser.add_argument('--ef', type=str, default='default', choices=['default', 'openai', 'azure'], help='Embedding function to use (default or openai).')
    args = parser.parse_args()

    # Select the embedding function based on the user input
    if args.ef == "openai":
        embedding_function = openai_ef
    elif args.ef == "azure":
        embedding_function = azure_ef
    else:
        embedding_function = default_ef

    # Initialize the collection with the selected embedding function
    collection = chroma_client.get_or_create_collection(
        name=collection_name,
        embedding_function=embedding_function
    )

    # Perform the search
    results = search_in_chromadb(args.query, args.n, collection)

# Print the results
for i in range(len(results['documents'])):  # Iterate over each set of documents (outer list)
    for j in range(len(results['documents'][i])):  # Iterate over documents in each set (inner list)
        print(f"URL: {results['metadatas'][i][j]['url']}")
        print(f"Content: {results['documents'][i][j]}")  # Print the document
        print("-" * 50)
~~~
### This is END of the file search.py!

### This is START of the populate.py file:
~~~
from SynthSpider import fetch_sitemap, parse_sitemap, fetch_and_save_html, default_ef, openai_ef, azure_ef, chroma_client, collection_name
import argparse
import asyncio
import logging
from tqdm import tqdm

async def main(sitemap_url, n, ef):
    global collection
    """
    Main function to fetch, parse the sitemap, and save HTML content to ChromaDB.
    """
    # Select the embedding function based on the user input
    if ef == "openai":
        embedding_function = openai_ef
    elif ef == "azure":
        embedding_function = azure_ef
    else:
        embedding_function = default_ef

    # Get or create the collection with the selected embedding function
    collection = chroma_client.get_or_create_collection(
        name=collection_name,
        embedding_function=embedding_function
    )

    # Fetch the sitemap
    sitemap_xml = await fetch_sitemap(sitemap_url)
    if not sitemap_xml:
        logging.error("Failed to fetch sitemap.")
        return
    else:
        print(f"Successfully fetched: {sitemap_url}")

    # Parse the sitemap to get URLs
    urls = parse_sitemap(sitemap_xml, n)
    if not urls:
        logging.error("No URLs found in sitemap.")
        return

    # Set up the progress bar
    pbar = tqdm(total=len(urls))

    # Function to update the progress bar
    def update_progress():
        pbar.update(1)

    # Fetch and save HTML content of each URL
    tasks = [fetch_and_save_html(url, update_progress, collection) for url in urls]
    await asyncio.gather(*tasks)
    
    pbar.close()

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Fetch Sitemap Content.')
    parser.add_argument('sitemap_url', type=str, help='Full URL of sitemap.xml file.')
    parser.add_argument('--n', type=int, default=None, help='Number of results to return.')
    parser.add_argument('--ef', type=str, default='default', choices=['default', 'openai', 'azure'], help='Embedding function to use (default or openai).')
    args = parser.parse_args()

    asyncio.run(main(args.sitemap_url, args.n, args.ef))
~~~
### This is END of the file populate.py!

